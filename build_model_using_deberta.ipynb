{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe9e9ee",
   "metadata": {},
   "source": [
    "## Please run this code only if you want to regenerate the embedding file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f9be2",
   "metadata": {},
   "source": [
    "### Refer to pytorch documentation to find the pytorch version that matches with your CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a469b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DebertaTokenizer, DebertaModel\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e49a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"books_data.csv\", nrows=40000)\n",
    "data['Title'] = data['Title'].fillna('Unknown')\n",
    "data['categories'] = data['categories'].fillna('Unknown')\n",
    "data['description'] = data['description'].fillna('')\n",
    "data['description'] = data['description'].apply(lambda x: x.lower())\n",
    "data['book_content'] = (\n",
    "    (data['Title'] + ' ') * 2\n",
    "    + data['description'] + ' '\n",
    "    + data['authors'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '') + ' '\n",
    "    + data['categories'].apply(lambda x: ' '.join(x) * 5 if isinstance(x, list) else '')\n",
    ")\n",
    "data['book_content'] = data['book_content'].str.replace(r'[^\\w\\s]', '', regex=True).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac14c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12454    sniper one iron brigade series sniper one iron...\n",
      "4837     stranger from arizona stranger from arizona wh...\n",
      "21165    don juan tenorio mr juan tenorio spanish editi...\n",
      "24083    surviving sisters surviving sisters why do bro...\n",
      "37168    familiar christmas fear familiar book 11 harle...\n",
      "Name: book_content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['book_content'].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642ea109",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "deberta_model = DebertaModel.from_pretrained('microsoft/deberta-base').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f766b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, max_len=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "tokenized_data = tokenize_texts(data['book_content'], max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc06129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40000, 768])\n"
     ]
    }
   ],
   "source": [
    "def generate_deberta_embeddings_in_batches(tokenized_data, deberta_model, batch_size=32, device='cuda'):\n",
    "    # Move the model to the specified device (GPU or CPU)\n",
    "    deberta_model = deberta_model.to(device)\n",
    "    \n",
    "    # Initialize an empty list to store the embeddings\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Calculate total batches\n",
    "    total_samples = tokenized_data['input_ids'].shape[0]\n",
    "    \n",
    "    for start_idx in range(0, total_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_samples)\n",
    "        \n",
    "        # Slice batch input_ids and attention_mask\n",
    "        input_ids_batch = tokenized_data['input_ids'][start_idx:end_idx].to(device)  # Move to the same device\n",
    "        attention_mask_batch = tokenized_data['attention_mask'][start_idx:end_idx].to(device)  # Move to the same device\n",
    "\n",
    "        # Get DeBERTa embeddings without computing gradients\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = deberta_model(input_ids=input_ids_batch, attention_mask=attention_mask_batch).last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        # Move embeddings back to CPU to save GPU memory\n",
    "        all_embeddings.append(batch_embeddings.cpu())\n",
    "        \n",
    "        # Optionally clear cache to free memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all batch embeddings into a single tensor\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "deberta_embeddings = generate_deberta_embeddings_in_batches(tokenized_data, deberta_model, batch_size=32, device=device)\n",
    "print(deberta_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "423d5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, data, deberta_embeddings):\n",
    "        self.data = data\n",
    "        self.deberta_embeddings = deberta_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        book1_emb = self.deberta_embeddings[idx]\n",
    "        book2_emb = self.deberta_embeddings[(idx + 1) % len(self.data)]  # Pair with next item\n",
    "        label = 1 if self.data['categories'].iloc[idx] == self.data['categories'].iloc[(idx + 1) % len(self.data)] else 0\n",
    "        return book1_emb, book2_emb, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2982375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBranch, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 512)  # First dense layer, increased number of units\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Dropout to prevent overfitting\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)  # Second dense layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)  # Dropout again\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 128)  # Third dense layer (matches original)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb9f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn_branch = CustomBranch()\n",
    "        self.fc1 = nn.Linear(768, 128)  # Assuming DeBERTa output size is 1024\n",
    "        self.fc2 = nn.Linear(128, 64)   # Reducing to 64 dimensions\n",
    "        self.fc3 = nn.Linear(64 * 2, 2)  # Concatenating two 64-dim vectors, and output size 2 for binary classification\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        concatenated = torch.cat((output1, output2), dim=1)\n",
    "        output = self.fc3(concatenated)\n",
    "        return output\n",
    "\n",
    "# Dataset and DataLoader\n",
    "pair_dataset = PairDataset(data, deberta_embeddings)\n",
    "pair_loader = DataLoader(pair_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the Siamese model, loss function, and optimizer\n",
    "siamese_model = SiameseNetwork().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(siamese_model.parameters(), lr=1e-5, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72eb34ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.3635\n",
      "Epoch [2/100], Loss: 0.1696\n",
      "Epoch [3/100], Loss: 0.1667\n",
      "Epoch [4/100], Loss: 0.1657\n",
      "Epoch [5/100], Loss: 0.1648\n",
      "Epoch [6/100], Loss: 0.1639\n",
      "Epoch [7/100], Loss: 0.1631\n",
      "Epoch [8/100], Loss: 0.1623\n",
      "Epoch [9/100], Loss: 0.1616\n",
      "Epoch [10/100], Loss: 0.1610\n",
      "Epoch [11/100], Loss: 0.1605\n",
      "Epoch [12/100], Loss: 0.1601\n",
      "Epoch [13/100], Loss: 0.1597\n",
      "Epoch [14/100], Loss: 0.1594\n",
      "Epoch [15/100], Loss: 0.1592\n",
      "Epoch [16/100], Loss: 0.1589\n",
      "Epoch [17/100], Loss: 0.1588\n",
      "Epoch [18/100], Loss: 0.1586\n",
      "Epoch [19/100], Loss: 0.1584\n",
      "Epoch [20/100], Loss: 0.1582\n",
      "Epoch [21/100], Loss: 0.1582\n",
      "Epoch [22/100], Loss: 0.1581\n",
      "Epoch [23/100], Loss: 0.1580\n",
      "Epoch [24/100], Loss: 0.1578\n",
      "Epoch [25/100], Loss: 0.1577\n",
      "Epoch [26/100], Loss: 0.1576\n",
      "Epoch [27/100], Loss: 0.1575\n",
      "Epoch [28/100], Loss: 0.1574\n",
      "Epoch [29/100], Loss: 0.1572\n",
      "Epoch [30/100], Loss: 0.1571\n",
      "Epoch [31/100], Loss: 0.1570\n",
      "Epoch [32/100], Loss: 0.1569\n",
      "Epoch [33/100], Loss: 0.1568\n",
      "Epoch [34/100], Loss: 0.1566\n",
      "Epoch [35/100], Loss: 0.1565\n",
      "Epoch [36/100], Loss: 0.1564\n",
      "Epoch [37/100], Loss: 0.1563\n",
      "Epoch [38/100], Loss: 0.1561\n",
      "Epoch [39/100], Loss: 0.1561\n",
      "Epoch [40/100], Loss: 0.1560\n",
      "Epoch [41/100], Loss: 0.1558\n",
      "Epoch [42/100], Loss: 0.1558\n",
      "Epoch [43/100], Loss: 0.1556\n",
      "Epoch [44/100], Loss: 0.1555\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1553\n",
      "Epoch [47/100], Loss: 0.1552\n",
      "Epoch [48/100], Loss: 0.1551\n",
      "Epoch [49/100], Loss: 0.1550\n",
      "Epoch [50/100], Loss: 0.1549\n",
      "Epoch [51/100], Loss: 0.1548\n",
      "Epoch [52/100], Loss: 0.1547\n",
      "Epoch [53/100], Loss: 0.1547\n",
      "Epoch [54/100], Loss: 0.1546\n",
      "Epoch [55/100], Loss: 0.1544\n",
      "Epoch [56/100], Loss: 0.1543\n",
      "Epoch [57/100], Loss: 0.1543\n",
      "Epoch [58/100], Loss: 0.1541\n",
      "Epoch [59/100], Loss: 0.1541\n",
      "Epoch [60/100], Loss: 0.1540\n",
      "Epoch [61/100], Loss: 0.1538\n",
      "Epoch [62/100], Loss: 0.1538\n",
      "Epoch [63/100], Loss: 0.1537\n",
      "Epoch [64/100], Loss: 0.1536\n",
      "Epoch [65/100], Loss: 0.1535\n",
      "Epoch [66/100], Loss: 0.1534\n",
      "Epoch [67/100], Loss: 0.1533\n",
      "Epoch [68/100], Loss: 0.1533\n",
      "Epoch [69/100], Loss: 0.1531\n",
      "Epoch [70/100], Loss: 0.1531\n",
      "Epoch [71/100], Loss: 0.1530\n",
      "Epoch [72/100], Loss: 0.1529\n",
      "Epoch [73/100], Loss: 0.1528\n",
      "Epoch [74/100], Loss: 0.1527\n",
      "Epoch [75/100], Loss: 0.1528\n",
      "Epoch [76/100], Loss: 0.1526\n",
      "Epoch [77/100], Loss: 0.1525\n",
      "Epoch [78/100], Loss: 0.1525\n",
      "Epoch [79/100], Loss: 0.1524\n",
      "Epoch [80/100], Loss: 0.1523\n",
      "Epoch [81/100], Loss: 0.1523\n",
      "Epoch [82/100], Loss: 0.1521\n",
      "Epoch [83/100], Loss: 0.1521\n",
      "Epoch [84/100], Loss: 0.1520\n",
      "Epoch [85/100], Loss: 0.1519\n",
      "Epoch [86/100], Loss: 0.1518\n",
      "Epoch [87/100], Loss: 0.1517\n",
      "Epoch [88/100], Loss: 0.1516\n",
      "Epoch [89/100], Loss: 0.1516\n",
      "Epoch [90/100], Loss: 0.1516\n",
      "Epoch [91/100], Loss: 0.1514\n",
      "Epoch [92/100], Loss: 0.1514\n",
      "Epoch [93/100], Loss: 0.1513\n",
      "Epoch [94/100], Loss: 0.1513\n",
      "Epoch [95/100], Loss: 0.1511\n",
      "Epoch [96/100], Loss: 0.1512\n",
      "Epoch [97/100], Loss: 0.1510\n",
      "Epoch [98/100], Loss: 0.1509\n",
      "Epoch [99/100], Loss: 0.1508\n",
      "Epoch [100/100], Loss: 0.1508\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    siamese_model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in pair_loader:\n",
    "        book1_emb, book2_emb, labels = batch\n",
    "        book1_emb, book2_emb, labels = book1_emb.cuda(), book2_emb.cuda(), labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = siamese_model(book1_emb, book2_emb)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(pair_loader):.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(siamese_model.state_dict(), 'siamese_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c7122a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_from_model(deberta_embeddings, siamese_model):\n",
    "    siamese_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        book_embeddings = siamese_model.cnn_branch(deberta_embeddings.cuda())  # Pass through CNN branch\n",
    "    return book_embeddings.cpu().detach().numpy()  # Move to CPU and detach from computation graph\n",
    "\n",
    "# Use the function to get the processed embeddings\n",
    "book_embeddings = extract_embeddings_from_model(deberta_embeddings, siamese_model)\n",
    "\n",
    "# Save the embeddings for future use\n",
    "torch.save(book_embeddings, 'deberta_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4e201",
   "metadata": {},
   "source": [
    "### Dumps cosine similarities matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e17ad830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# folder_path = r'dumped_matrices/chebyshev_distance'\n",
    "# os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# # Save the matrix in chunks\n",
    "# chunk_size = 2048\n",
    "# num_chunks = len(manhattan_dist_matrix) // chunk_size + 1\n",
    "\n",
    "# for i in range(num_chunks):\n",
    "#     chunk = manhattan_dist_matrix[i * chunk_size: (i + 1) * chunk_size]\n",
    "#     file_path = os.path.join(folder_path, f'chebyshev_matrix_chunk_{i}.pkl')\n",
    "#     with open(file_path, 'wb') as f:\n",
    "#         pickle.dump(chunk, f)\n",
    "#     clear_output(wait=True)\n",
    "#     print(f'Saved {i} / {num_chunks} chunks to {file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
