{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86c1c4a",
   "metadata": {},
   "source": [
    "## Please run this code only if you want to regenerate the embedding file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d3aa5",
   "metadata": {},
   "source": [
    "### Refer to pytorch documentation to find the pytorch version that matches with your CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a469b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e49a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"books_data.csv\", nrows=40000)\n",
    "data['Title'] = data['Title'].fillna('Unknown')\n",
    "data['categories'] = data['categories'].fillna('Unknown')\n",
    "data['description'] = data['description'].fillna('')\n",
    "data['description'] = data['description'].apply(lambda x: x.lower())\n",
    "data['book_content'] = (\n",
    "    (data['Title'] + ' ') * 2\n",
    "    + data['description'] + ' '\n",
    "    + data['authors'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '') + ' '\n",
    "    + data['categories'].apply(lambda x: ' '.join(x) * 5 if isinstance(x, list) else '')\n",
    ")\n",
    "data['book_content'] = data['book_content'].str.replace(r'[^\\w\\s]', '', regex=True).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac14c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16801    the complete home guide to herbs natural heali...\n",
      "23013    incredible voyage incredible voyage a welshman...\n",
      "172      the mask of priam the mask of priam a collecti...\n",
      "21648    passing on passing on passingon occurs when ha...\n",
      "26235    saint of auschwitz story of maksymilian kolbe ...\n",
      "Name: book_content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['book_content'].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642ea109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer and model\n",
    "logging.set_verbosity_error()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-large-uncased').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f766b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, max_len=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65579b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenize_texts(data['book_content'], max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc06129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_embeddings_in_batches(tokenized_data, bert_model, batch_size=32, device='cuda'):\n",
    "    # Move the model to the specified device (GPU or CPU)\n",
    "    bert_model = bert_model.to(device)\n",
    "    \n",
    "    # Initialize an empty list to store the embeddings\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Calculate total batches\n",
    "    total_samples = tokenized_data['input_ids'].shape[0]\n",
    "    \n",
    "    for start_idx in range(0, total_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_samples)\n",
    "        \n",
    "        # Slice batch input_ids and attention_mask\n",
    "        input_ids_batch = tokenized_data['input_ids'][start_idx:end_idx].to(device)  # Move to the same device\n",
    "        attention_mask_batch = tokenized_data['attention_mask'][start_idx:end_idx].to(device)  # Move to the same device\n",
    "\n",
    "        # Get BERT embeddings without computing gradients\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = bert_model(input_ids=input_ids_batch, attention_mask=attention_mask_batch)[1]\n",
    "        \n",
    "        # Move embeddings back to CPU to save GPU memory\n",
    "        all_embeddings.append(batch_embeddings.cpu())\n",
    "        \n",
    "        # Optionally clear cache to free memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all batch embeddings into a single tensor\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Use the function to generate embeddings in batches\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_embeddings = generate_bert_embeddings_in_batches(tokenized_data, bert_model, batch_size=32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423d5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, data, bert_embeddings):\n",
    "        self.data = data\n",
    "        self.bert_embeddings = bert_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        book1_emb = self.bert_embeddings[idx]\n",
    "        book2_emb = self.bert_embeddings[(idx + 1) % len(self.data)]  # Pair with next item\n",
    "        label = 1 if self.data['categories'].iloc[idx] == self.data['categories'].iloc[(idx + 1) % len(self.data)] else 0\n",
    "        return book1_emb, book2_emb, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2982375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBranch, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)  # First dense layer, increased number of units\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Dropout to prevent overfitting\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)  # Second dense layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)  # Dropout again\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 128)  # Third dense layer (matches original)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb9f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn_branch = CustomBranch()\n",
    "        self.fc1 = nn.Linear(1024, 128)  # Assuming BERT output size is 768\n",
    "        self.fc2 = nn.Linear(128, 64)   # Reducing to 64 dimensions\n",
    "        self.fc3 = nn.Linear(64 * 2, 2)  # Concatenating two 64-dim vectors, and output size 2 for binary classification\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        concatenated = torch.cat((output1, output2), dim=1)\n",
    "        output = self.fc3(concatenated)\n",
    "        return output\n",
    "\n",
    "# Dataset and DataLoader\n",
    "pair_dataset = PairDataset(data, bert_embeddings)\n",
    "pair_loader = DataLoader(pair_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the Siamese model, loss function, and optimizer\n",
    "siamese_model = SiameseNetwork().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(siamese_model.parameters(), lr=1e-5, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72eb34ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.2270\n",
      "Epoch [2/100], Loss: 0.1659\n",
      "Epoch [3/100], Loss: 0.1654\n",
      "Epoch [4/100], Loss: 0.1651\n",
      "Epoch [5/100], Loss: 0.1649\n",
      "Epoch [6/100], Loss: 0.1648\n",
      "Epoch [7/100], Loss: 0.1647\n",
      "Epoch [8/100], Loss: 0.1646\n",
      "Epoch [9/100], Loss: 0.1644\n",
      "Epoch [10/100], Loss: 0.1647\n",
      "Epoch [11/100], Loss: 0.1645\n",
      "Epoch [12/100], Loss: 0.1643\n",
      "Epoch [13/100], Loss: 0.1642\n",
      "Epoch [14/100], Loss: 0.1642\n",
      "Epoch [15/100], Loss: 0.1641\n",
      "Epoch [16/100], Loss: 0.1641\n",
      "Epoch [17/100], Loss: 0.1640\n",
      "Epoch [18/100], Loss: 0.1641\n",
      "Epoch [19/100], Loss: 0.1638\n",
      "Epoch [20/100], Loss: 0.1639\n",
      "Epoch [21/100], Loss: 0.1639\n",
      "Epoch [22/100], Loss: 0.1637\n",
      "Epoch [23/100], Loss: 0.1636\n",
      "Epoch [24/100], Loss: 0.1636\n",
      "Epoch [25/100], Loss: 0.1636\n",
      "Epoch [26/100], Loss: 0.1636\n",
      "Epoch [27/100], Loss: 0.1634\n",
      "Epoch [28/100], Loss: 0.1634\n",
      "Epoch [29/100], Loss: 0.1634\n",
      "Epoch [30/100], Loss: 0.1633\n",
      "Epoch [31/100], Loss: 0.1633\n",
      "Epoch [32/100], Loss: 0.1632\n",
      "Epoch [33/100], Loss: 0.1631\n",
      "Epoch [34/100], Loss: 0.1631\n",
      "Epoch [35/100], Loss: 0.1631\n",
      "Epoch [36/100], Loss: 0.1629\n",
      "Epoch [37/100], Loss: 0.1630\n",
      "Epoch [38/100], Loss: 0.1630\n",
      "Epoch [39/100], Loss: 0.1627\n",
      "Epoch [40/100], Loss: 0.1630\n",
      "Epoch [41/100], Loss: 0.1629\n",
      "Epoch [42/100], Loss: 0.1625\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1627\n",
      "Epoch [45/100], Loss: 0.1623\n",
      "Epoch [46/100], Loss: 0.1624\n",
      "Epoch [47/100], Loss: 0.1624\n",
      "Epoch [48/100], Loss: 0.1622\n",
      "Epoch [49/100], Loss: 0.1621\n",
      "Epoch [50/100], Loss: 0.1621\n",
      "Epoch [51/100], Loss: 0.1619\n",
      "Epoch [52/100], Loss: 0.1620\n",
      "Epoch [53/100], Loss: 0.1618\n",
      "Epoch [54/100], Loss: 0.1620\n",
      "Epoch [55/100], Loss: 0.1619\n",
      "Epoch [56/100], Loss: 0.1619\n",
      "Epoch [57/100], Loss: 0.1617\n",
      "Epoch [58/100], Loss: 0.1617\n",
      "Epoch [59/100], Loss: 0.1617\n",
      "Epoch [60/100], Loss: 0.1617\n",
      "Epoch [61/100], Loss: 0.1615\n",
      "Epoch [62/100], Loss: 0.1614\n",
      "Epoch [63/100], Loss: 0.1613\n",
      "Epoch [64/100], Loss: 0.1615\n",
      "Epoch [65/100], Loss: 0.1613\n",
      "Epoch [66/100], Loss: 0.1611\n",
      "Epoch [67/100], Loss: 0.1611\n",
      "Epoch [68/100], Loss: 0.1609\n",
      "Epoch [69/100], Loss: 0.1610\n",
      "Epoch [70/100], Loss: 0.1610\n",
      "Epoch [71/100], Loss: 0.1606\n",
      "Epoch [72/100], Loss: 0.1608\n",
      "Epoch [73/100], Loss: 0.1607\n",
      "Epoch [74/100], Loss: 0.1605\n",
      "Epoch [75/100], Loss: 0.1606\n",
      "Epoch [76/100], Loss: 0.1606\n",
      "Epoch [77/100], Loss: 0.1605\n",
      "Epoch [78/100], Loss: 0.1604\n",
      "Epoch [79/100], Loss: 0.1603\n",
      "Epoch [80/100], Loss: 0.1603\n",
      "Epoch [81/100], Loss: 0.1602\n",
      "Epoch [82/100], Loss: 0.1601\n",
      "Epoch [83/100], Loss: 0.1600\n",
      "Epoch [84/100], Loss: 0.1600\n",
      "Epoch [85/100], Loss: 0.1600\n",
      "Epoch [86/100], Loss: 0.1600\n",
      "Epoch [87/100], Loss: 0.1598\n",
      "Epoch [88/100], Loss: 0.1598\n",
      "Epoch [89/100], Loss: 0.1598\n",
      "Epoch [90/100], Loss: 0.1598\n",
      "Epoch [91/100], Loss: 0.1597\n",
      "Epoch [92/100], Loss: 0.1594\n",
      "Epoch [93/100], Loss: 0.1596\n",
      "Epoch [94/100], Loss: 0.1595\n",
      "Epoch [95/100], Loss: 0.1594\n",
      "Epoch [96/100], Loss: 0.1597\n",
      "Epoch [97/100], Loss: 0.1594\n",
      "Epoch [98/100], Loss: 0.1594\n",
      "Epoch [99/100], Loss: 0.1593\n",
      "Epoch [100/100], Loss: 0.1594\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    siamese_model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in pair_loader:\n",
    "        book1_emb, book2_emb, labels = batch\n",
    "        book1_emb, book2_emb, labels = book1_emb.cuda(), book2_emb.cuda(), labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = siamese_model(book1_emb, book2_emb)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(pair_loader):.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(siamese_model.state_dict(), 'siamese_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c7122a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract book embeddings from the CNN branch of the Siamese model\n",
    "def extract_embeddings_from_model(bert_embeddings, siamese_model):\n",
    "    siamese_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        book_embeddings = siamese_model.cnn_branch(bert_embeddings.cuda())  # Pass through CNN branch\n",
    "    return book_embeddings.cpu().detach().numpy()  # Move to CPU and detach from computation graph\n",
    "\n",
    "# Use the function to get the processed embeddings\n",
    "book_embeddings = extract_embeddings_from_model(bert_embeddings, siamese_model)\n",
    "\n",
    "# Save the embeddings for future use\n",
    "torch.save(book_embeddings, 'bert_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4e201",
   "metadata": {},
   "source": [
    "### Dumps cosine similarities matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e17ad830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# folder_path = r'dumped_matrices/chebyshev_distance'\n",
    "# os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# # Save the matrix in chunks\n",
    "# chunk_size = 2048\n",
    "# num_chunks = len(manhattan_dist_matrix) // chunk_size + 1\n",
    "\n",
    "# for i in range(num_chunks):\n",
    "#     chunk = manhattan_dist_matrix[i * chunk_size: (i + 1) * chunk_size]\n",
    "#     file_path = os.path.join(folder_path, f'chebyshev_matrix_chunk_{i}.pkl')\n",
    "#     with open(file_path, 'wb') as f:\n",
    "#         pickle.dump(chunk, f)\n",
    "#     clear_output(wait=True)\n",
    "#     print(f'Saved {i} / {num_chunks} chunks to {file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
