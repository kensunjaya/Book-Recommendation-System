{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a469b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from IPython.display import clear_output\n",
    "from transformers import logging\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e49a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"books_data.csv\", nrows=20000)\n",
    "data['Title'] = data['Title'].fillna('Unknown')\n",
    "data['categories'] = data['categories'].fillna('Unknown')\n",
    "data['description'] = data['description'].fillna('')\n",
    "data['description'] = data['description'].apply(lambda x: x.lower())\n",
    "data['book_content'] = (data['Title'] + ' ') * 2 + ' ' + data['description'] + ' ' + data['authors'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "data['book_content'] = data['book_content'].str.replace(r'[^\\w\\s]', '', regex=True).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ea109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer and model\n",
    "logging.set_verbosity_error()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f766b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Tokenize the book content using BERT tokenizer\n",
    "def tokenize_texts(texts, max_len=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenize_texts(data['book_content'], max_len=128)\n",
    "tokenized_data_length = len(tokenized_data['input_ids'])\n",
    "\n",
    "def batch_bert_embeddings(data, batch_size=32, max_len=128):\n",
    "    # Initialize empty list to store all embeddings\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Iterate over the dataset in batches\n",
    "    for start in range(0, len(data), batch_size):\n",
    "        end = min(start + batch_size, len(data))\n",
    "        batch_texts = data['book_content'][start:end]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        tokenized_data = tokenizer(\n",
    "            batch_texts.tolist(),\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        \n",
    "        # Generate BERT embeddings for this batch\n",
    "        batch_embeddings = bert_model(\n",
    "            tokenized_data['input_ids'], \n",
    "            attention_mask=tokenized_data['attention_mask']\n",
    "        )[1]\n",
    "        \n",
    "        # Append to the list of all embeddings\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Processed {end} / {tokenized_data_length}  | {end / tokenized_data_length * 100:.2f}%\")\n",
    "    \n",
    "    # Concatenate all embeddings into a single tensor\n",
    "    return tf.concat(all_embeddings, axis=0)\n",
    "\n",
    "# Use the function to get embeddings in batches\n",
    "bert_embeddings = batch_bert_embeddings(data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TensorFlow tensor to NumPy array\n",
    "bert_embeddings_np = bert_embeddings.numpy()\n",
    "\n",
    "# Save the embeddings to a .npy file\n",
    "np.save('bert_embeddings.npy', bert_embeddings_np)\n",
    "\n",
    "print(\"Embeddings saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72eb34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairs of books and labels (1 for similar, 0 for dissimilar based on 'categories')\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class PairGenerator(Sequence):\n",
    "    def __init__(self, data, bert_embeddings, batch_size=32):\n",
    "        self.data = data\n",
    "        self.bert_embeddings = bert_embeddings\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = list(range(len(data)))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate one batch of pairs and labels\n",
    "        pairs = []\n",
    "        labels = []\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        for i in batch_indices:\n",
    "            for j in batch_indices:\n",
    "                if i != j:\n",
    "                    if self.data['categories'].iloc[i] == self.data['categories'].iloc[j]:\n",
    "                        labels.append(1)\n",
    "                    else:\n",
    "                        labels.append(0)\n",
    "                    pairs.append((self.bert_embeddings[i], self.bert_embeddings[j]))\n",
    "\n",
    "        X_train_1 = np.array([p[0] for p in pairs])\n",
    "        X_train_2 = np.array([p[1] for p in pairs])\n",
    "        return (X_train_1, X_train_2), np.array(labels)\n",
    "\n",
    "# Create the pair generator for training\n",
    "pair_gen = PairGenerator(data, bert_embeddings, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c037b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model to process BERT embeddings\n",
    "def create_cnn_model():\n",
    "    input_layer = Input(shape=(768,))  # BERT's pooled output has 768 dimensions\n",
    "    dense_layer = Dense(128, activation='relu')(input_layer)\n",
    "    return Model(inputs=input_layer, outputs=dense_layer)\n",
    "\n",
    "cnn_branch = create_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Siamese network using the CNN model\n",
    "input_1 = Input(shape=(768,))  # BERT embeddings for the first book\n",
    "input_2 = Input(shape=(768,))  # BERT embeddings for the second book\n",
    "\n",
    "# Pass both inputs through the shared CNN model\n",
    "output_1 = cnn_branch(input_1)\n",
    "output_2 = cnn_branch(input_2)\n",
    "\n",
    "# Concatenate outputs and add Dense layers for classification\n",
    "concatenated = Concatenate()([output_1, output_2])\n",
    "dense_layer1 = Dense(128, activation='relu')(concatenated)\n",
    "dense_layer2 = Dense(64, activation='relu')(dense_layer1)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_layer2)\n",
    "\n",
    "# Build and compile the Siamese model\n",
    "siamese_model = Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the Siamese network\n",
    "\n",
    "siamese_model.fit(pair_gen, epochs=10)\n",
    "\n",
    "# Generate BERT embeddings for all books to compute similarity\n",
    "book_embeddings = cnn_branch.predict(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7122a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.save('siamese_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6876a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_book_embeddings = normalize(book_embeddings)\n",
    "cosine_sim_matrix = cosine_similarity(normalized_book_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88cd5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the book recommendation function based on cosine similarity\n",
    "def recommend_books(book_title, threshold, cosine_sim_matrix):\n",
    "    # Get the index of the book that matches the title\n",
    "    idx = data[data['Title'] == book_title].index[0]\n",
    "\n",
    "    # Get the cosine similarity scores for all books with this book\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "\n",
    "    # Sort the books based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = [(i, \"{:.5f}\".format(score)) for i, score in sim_scores if score >= threshold]\n",
    "\n",
    "    # Get the book titles and their similarity scores\n",
    "    book_recommendations = [(data['Title'].iloc[i[0]], i[1]) for i in sim_scores]\n",
    "\n",
    "    return book_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5560ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Compute Euclidean distance matrix\n",
    "euclidean_dist_matrix = euclidean_distances(book_embeddings)\n",
    "\n",
    "# Define a recommendation function based on Euclidean distance\n",
    "def recommend_books_by_euclidean(book_title, threshold, euclidean_dist_matrix):\n",
    "    idx = data[data['Title'] == book_title].index[0]\n",
    "    \n",
    "    dist_scores = list(enumerate(euclidean_dist_matrix[idx]))\n",
    "    \n",
    "    # Sort the books based on Euclidean distance (lower is more similar)\n",
    "    dist_scores = sorted(dist_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    \n",
    "    # Filter recommendations based on threshold (optional)\n",
    "    dist_scores = [(i, \"{:.5f}\".format(score)) for i, score in dist_scores if score >= threshold]\n",
    "    recommendations = [(data['Title'].iloc[i[0]], \"{:.5f}\".format(score)) \n",
    "                       for i, score in dist_scores if score >= threshold]\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01740755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "book_title = ''\n",
    "while (book_title != 'q'):\n",
    "  clear_output(wait=True)\n",
    "  book_title = input(\"Enter the title of a book: \")\n",
    "  # recommended_books = recommend_books(book_title, threshold=0.1)\n",
    "  recommended_books = recommend_books(book_title, 0.5, cosine_sim_matrix)\n",
    "  f = open('output.txt', 'w')\n",
    "  f.write('Counts: ' + str(len(recommended_books)) + '\\n\\n')\n",
    "  for book in recommended_books:\n",
    "    f.write(book[1] + ' | ' + str(book[0]) + '\\n')\n",
    "\n",
    "  print('Found: ' + str(len(recommended_books)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
