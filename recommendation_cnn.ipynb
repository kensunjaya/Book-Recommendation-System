{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e49a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"books_data.csv\", nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2371e327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'description', 'authors', 'image', 'previewLink', 'publisher',\n",
       "       'publishedDate', 'infoLink', 'categories', 'ratingsCount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ce0f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Title          1000 non-null   object \n",
      " 1   description    728 non-null    object \n",
      " 2   authors        932 non-null    object \n",
      " 3   image          808 non-null    object \n",
      " 4   previewLink    972 non-null    object \n",
      " 5   publisher      690 non-null    object \n",
      " 6   publishedDate  963 non-null    object \n",
      " 7   infoLink       972 non-null    object \n",
      " 8   categories     858 non-null    object \n",
      " 9   ratingsCount   234 non-null    float64\n",
      "dtypes: float64(1), object(9)\n",
      "memory usage: 78.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd9577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'average_rating' to a numeric data type\n",
    "data['ratingsCount'] = pd.to_numeric(data['ratingsCount'], \n",
    "                                       errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b718fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'book_content' by combining 'title' and 'authors'\n",
    "data['book_content'] = (data['Title'] + ' ') * 2 + data['description'] + ' ' + data['authors'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4904a",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d28575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)  # Or any reasonable vocab size\n",
    "tokenizer.fit_on_texts(data['book_content'].values.astype('U'))\n",
    "\n",
    "# Convert the texts to sequences and pad them\n",
    "sequences = tokenizer.texts_to_sequences(data['book_content'].values.astype('U'))\n",
    "padded_sequences = pad_sequences(sequences, maxlen=500)  # Adjust maxlen as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eef6437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0109665",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a45360a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kenneth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(input_dim=5000, output_dim=128, input_length=500))\n",
    "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))  # Extra convolutional layer\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dense(64, activation='relu'))  # Extra Dense layer for more complex embeddings\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ac523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m7805/7805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m795s\u001b[0m 102ms/step - accuracy: 0.9609 - loss: 0.1317\n",
      "Epoch 2/5\n",
      "\u001b[1m7805/7805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m790s\u001b[0m 101ms/step - accuracy: 0.9710 - loss: 0.0875\n",
      "Epoch 3/5\n",
      "\u001b[1m7805/7805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m791s\u001b[0m 101ms/step - accuracy: 0.9730 - loss: 0.0803\n",
      "Epoch 4/5\n",
      "\u001b[1m7805/7805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m813s\u001b[0m 104ms/step - accuracy: 0.9746 - loss: 0.0753\n",
      "Epoch 5/5\n",
      "\u001b[1m7805/7805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m779s\u001b[0m 100ms/step - accuracy: 0.9747 - loss: 0.0736\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "data['categories'] = data['categories'].fillna('Unknown')  # Ensure 'categories' column exists and has no NaNs\n",
    "labels = []\n",
    "pairs = []\n",
    "\n",
    "# Create pairs of books and assign labels\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if data['categories'].iloc[i] == data['categories'].iloc[j]:\n",
    "            labels.append(1)  # Similar books (same categories)\n",
    "        else:\n",
    "            labels.append(0)  # Dissimilar books (different categoriess)\n",
    "        pairs.append((padded_sequences[i], padded_sequences[j]))\n",
    "\n",
    "# Convert pairs and labels to numpy arrays\n",
    "import numpy as np\n",
    "labels = np.array(labels)\n",
    "pairs = np.array(pairs)\n",
    "\n",
    "# Separate input into two arrays: one for each book in the pair\n",
    "X_train_1 = np.array([p[0] for p in pairs])\n",
    "X_train_2 = np.array([p[1] for p in pairs])\n",
    "\n",
    "# Define the CNN branch to be shared between both inputs\n",
    "def create_cnn_model():\n",
    "    input_layer = Input(shape=(500,))\n",
    "    embedding_layer = Embedding(input_dim=5000, output_dim=128, input_length=500)(input_layer)\n",
    "    conv_layer1 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "    conv_layer2 = Conv1D(filters=128, kernel_size=5, activation='relu')(conv_layer1)\n",
    "    pooling_layer = GlobalMaxPooling1D()(conv_layer2)\n",
    "    dense_layer = Dense(128, activation='relu')(pooling_layer)\n",
    "    return Model(inputs=input_layer, outputs=dense_layer)\n",
    "\n",
    "# Create two inputs for the pairs\n",
    "input_1 = Input(shape=(500,))\n",
    "input_2 = Input(shape=(500,))\n",
    "\n",
    "# Create the CNN branch and share it between both inputs\n",
    "cnn_branch = create_cnn_model()\n",
    "\n",
    "# Get the embeddings for both inputs\n",
    "output_1 = cnn_branch(input_1)\n",
    "output_2 = cnn_branch(input_2)\n",
    "\n",
    "# Concatenate the outputs and add the Dense layers\n",
    "concatenated = Concatenate()([output_1, output_2])\n",
    "dense_layer1 = Dense(128, activation='relu')(concatenated)\n",
    "dense_layer2 = Dense(64, activation='relu')(dense_layer1)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_layer2)\n",
    "\n",
    "# Build the Siamese model\n",
    "siamese_model = Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the pairs of sequences\n",
    "with tf.device('/GPU:0'):\n",
    "    siamese_model.fit([X_train_1, X_train_2], labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35bb9a",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45cc2573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "book_embeddings = siamese_model.predict([padded_sequences, padded_sequences])\n",
    "normalized_book_embeddings = normalize(book_embeddings)\n",
    "cosine_sim_matrix = cosine_similarity(normalized_book_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7711fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books(book_title, threshold, cosine_sim_matrix):\n",
    "    # Get the index of the book that matches the title\n",
    "    idx = data[data['Title'] == book_title].index[0]\n",
    "\n",
    "    # Get the cosine similarity scores for all books with this book\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "\n",
    "    # Sort the books based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = [(i, \"{:.5f}\".format(score)) for i, score in sim_scores[0:] if score >= threshold]\n",
    "\n",
    "    # Get the book titles and their similarity scores\n",
    "    book_recommendations = [(data['Title'].iloc[i[0]], i[1]) for i in sim_scores]\n",
    "\n",
    "    return book_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962f40a",
   "metadata": {},
   "source": [
    "### Recommend book using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01740755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "book_title = ''\n",
    "while (book_title != 'q'):\n",
    "  clear_output(wait=True)\n",
    "  book_title = input(\"Enter the title of a book: \")\n",
    "  # recommended_books = recommend_books(book_title, threshold=0.1)\n",
    "  recommended_books = recommend_books(book_title, 0.2, cosine_sim_matrix)\n",
    "  f = open('output.txt', 'w')\n",
    "  f.write('Counts: ' + str(len(recommended_books)) + '\\n\\n')\n",
    "  for book in recommended_books:\n",
    "    f.write(book[1] + ' | ' + str(book[0]) + '\\n')\n",
    "\n",
    "  print('Found: ' + str(len(recommended_books)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
