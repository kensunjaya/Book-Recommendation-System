{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef00355a",
   "metadata": {},
   "source": [
    "## Please run this code only if you want to regenerate the embedding file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd221dc3",
   "metadata": {},
   "source": [
    "### Refer to pytorch documentation to find the pytorch version that matches with your CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a469b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e49a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"books_data.csv\", nrows=40000)\n",
    "data['Title'] = data['Title'].fillna('Unknown')\n",
    "data['categories'] = data['categories'].fillna('Unknown')\n",
    "data['description'] = data['description'].fillna('')\n",
    "data['description'] = data['description'].apply(lambda x: x.lower())\n",
    "data['book_content'] = (\n",
    "    (data['Title'] + ' ') * 2\n",
    "    + data['description'] + ' '\n",
    "    + data['authors'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '') + ' '\n",
    "    + data['categories'].apply(lambda x: ' '.join(x) * 5 if isinstance(x, list) else '')\n",
    ")\n",
    "data['book_content'] = data['book_content'].str.replace(r'[^\\w\\s]', '', regex=True).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac14c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['book_content'].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ea109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer and model\n",
    "logging.set_verbosity_error()\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "roberta_model = roberta_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f766b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, tokenizer, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "tokenized_data = tokenize_data(data['Title'].tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc06129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_roberta_embeddings_in_batches(tokenized_data, roberta_model, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    total_samples = tokenized_data['input_ids'].shape[0]\n",
    "\n",
    "    for start_idx in range(0, total_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_samples)\n",
    "        input_ids_batch = tokenized_data['input_ids'][start_idx:end_idx].to('cuda')\n",
    "        attention_mask_batch = tokenized_data['attention_mask'][start_idx:end_idx].to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = roberta_model(input_ids=input_ids_batch, attention_mask=attention_mask_batch)[0]\n",
    "            batch_embeddings = batch_embeddings[:, 0, :]  # Extract CLS token representation\n",
    "\n",
    "        all_embeddings.append(batch_embeddings.cpu())\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "roberta_embeddings = generate_roberta_embeddings_in_batches(tokenized_data, roberta_model)\n",
    "print(roberta_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, data, bert_embeddings):\n",
    "        self.data = data\n",
    "        self.bert_embeddings = bert_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        book1_emb = self.bert_embeddings[idx]\n",
    "        book2_emb = self.bert_embeddings[(idx + 1) % len(self.data)]  # Pair with next item\n",
    "        label = 1 if self.data['categories'].iloc[idx] == self.data['categories'].iloc[(idx + 1) % len(self.data)] else 0\n",
    "        return book1_emb, book2_emb, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2982375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBranch, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 512)  # First dense layer, increased number of units\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Dropout to prevent overfitting\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)  # Second dense layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)  # Dropout again\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 128)  # Third dense layer (matches original)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn_branch = CustomBranch()\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64 * 2, 2)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        \n",
    "        concatenated = torch.cat((output1, output2), dim=1)\n",
    "\n",
    "        output = self.fc3(concatenated)\n",
    "        return output\n",
    "\n",
    "# Dataset and DataLoader\n",
    "pair_dataset = PairDataset(data, roberta_embeddings)\n",
    "pair_loader = DataLoader(pair_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "siamese_model = SiameseNetwork().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(siamese_model.parameters(), lr=1e-5, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    siamese_model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in pair_loader:\n",
    "        book1_emb, book2_emb, labels = batch\n",
    "        book1_emb, book2_emb, labels = book1_emb.cuda(), book2_emb.cuda(), labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = siamese_model(book1_emb, book2_emb)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(pair_loader):.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(siamese_model.state_dict(), 'siamese_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7122a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract book embeddings from the CNN branch of the Siamese model\n",
    "def extract_embeddings_from_model(roberta_embeddings, siamese_model):\n",
    "    siamese_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        book_embeddings = siamese_model.cnn_branch(roberta_embeddings.cuda())  # Pass through CNN branch\n",
    "    return book_embeddings.cpu().detach().numpy()  # Move to CPU and detach from computation graph\n",
    "\n",
    "# Use the function to get the processed embeddings\n",
    "book_embeddings = extract_embeddings_from_model(roberta_embeddings, siamese_model)\n",
    "\n",
    "# Save the embeddings for future use\n",
    "torch.save(book_embeddings, 'roberta_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4e201",
   "metadata": {},
   "source": [
    "### Dumps cosine similarities matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ad830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# folder_path = r'dumped_matrices/chebyshev_distance'\n",
    "# os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# # Save the matrix in chunks\n",
    "# chunk_size = 2048\n",
    "# num_chunks = len(manhattan_dist_matrix) // chunk_size + 1\n",
    "\n",
    "# for i in range(num_chunks):\n",
    "#     chunk = manhattan_dist_matrix[i * chunk_size: (i + 1) * chunk_size]\n",
    "#     file_path = os.path.join(folder_path, f'chebyshev_matrix_chunk_{i}.pkl')\n",
    "#     with open(file_path, 'wb') as f:\n",
    "#         pickle.dump(chunk, f)\n",
    "#     clear_output(wait=True)\n",
    "#     print(f'Saved {i} / {num_chunks} chunks to {file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
