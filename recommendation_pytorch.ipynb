{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a469b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import clear_output\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e49a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"books_data.csv\", nrows=50000)\n",
    "data['Title'] = data['Title'].fillna('Unknown')\n",
    "data['categories'] = data['categories'].fillna('Unknown')\n",
    "data['description'] = data['description'].fillna('')\n",
    "data['description'] = data['description'].apply(lambda x: x.lower())\n",
    "data['book_content'] = data['Title'] + ' ' + data['description'] + ' ' + data['authors'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '') + ' ' + data['categories'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "data['book_content'] = data['book_content'].str.replace(r'[^\\w\\s]', '', regex=True).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac14c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055                                 the book of garlic   \n",
      "22070    payette associates mas vi master architect ser...\n",
      "34684                               expediente x ruinas   \n",
      "41155    planting a bible garden a good book practical ...\n",
      "49861    amazing space facts a golden looklook book a b...\n",
      "Name: book_content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['book_content'].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642ea109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer and model\n",
    "logging.set_verbosity_error()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f766b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, max_len=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65579b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenize_texts(data['book_content'], max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc06129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_embeddings_in_batches(tokenized_data, bert_model, batch_size=32, device='cuda'):\n",
    "    # Move the model to the specified device (GPU or CPU)\n",
    "    bert_model = bert_model.to(device)\n",
    "    \n",
    "    # Initialize an empty list to store the embeddings\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Calculate total batches\n",
    "    total_samples = tokenized_data['input_ids'].shape[0]\n",
    "    \n",
    "    for start_idx in range(0, total_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_samples)\n",
    "        \n",
    "        # Slice batch input_ids and attention_mask\n",
    "        input_ids_batch = tokenized_data['input_ids'][start_idx:end_idx].to(device)  # Move to the same device\n",
    "        attention_mask_batch = tokenized_data['attention_mask'][start_idx:end_idx].to(device)  # Move to the same device\n",
    "\n",
    "        # Get BERT embeddings without computing gradients\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = bert_model(input_ids=input_ids_batch, attention_mask=attention_mask_batch)[1]\n",
    "        \n",
    "        # Move embeddings back to CPU to save GPU memory\n",
    "        all_embeddings.append(batch_embeddings.cpu())\n",
    "        \n",
    "        # Optionally clear cache to free memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all batch embeddings into a single tensor\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Use the function to generate embeddings in batches\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_embeddings = generate_bert_embeddings_in_batches(tokenized_data, bert_model, batch_size=32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423d5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, data, bert_embeddings):\n",
    "        self.data = data\n",
    "        self.bert_embeddings = bert_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        book1_emb = self.bert_embeddings[idx]\n",
    "        book2_emb = self.bert_embeddings[(idx + 1) % len(self.data)]  # Pair with next item\n",
    "        label = 1 if self.data['categories'].iloc[idx] == self.data['categories'].iloc[(idx + 1) % len(self.data)] else 0\n",
    "        return book1_emb, book2_emb, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2982375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNBranch, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 512)  # First dense layer, increased number of units\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Dropout to prevent overfitting\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)  # Second dense layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)  # Dropout again\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 128)  # Third dense layer (matches original)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb9f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn_branch = CNNBranch()\n",
    "        self.fc1 = nn.Linear(768, 128)  # Assuming BERT output size is 768\n",
    "        self.fc2 = nn.Linear(128, 64)   # Reducing to 64 dimensions\n",
    "        self.fc3 = nn.Linear(64 * 2, 2)  # Concatenating two 64-dim vectors, and output size 2 for binary classification\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        concatenated = torch.cat((output1, output2), dim=1)  # Concatenating along the feature dimension (dim=1)\n",
    "        output = self.fc3(concatenated)\n",
    "        return output\n",
    "\n",
    "# Dataset and DataLoader\n",
    "pair_dataset = PairDataset(data, bert_embeddings)\n",
    "pair_loader = DataLoader(pair_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the Siamese model, loss function, and optimizer\n",
    "siamese_model = SiameseNetwork().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(siamese_model.parameters(), lr=1e-5, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72eb34ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.2745\n",
      "Epoch [2/100], Loss: 0.1724\n",
      "Epoch [3/100], Loss: 0.1718\n",
      "Epoch [4/100], Loss: 0.1719\n",
      "Epoch [5/100], Loss: 0.1713\n",
      "Epoch [6/100], Loss: 0.1707\n",
      "Epoch [7/100], Loss: 0.1705\n",
      "Epoch [8/100], Loss: 0.1700\n",
      "Epoch [9/100], Loss: 0.1698\n",
      "Epoch [10/100], Loss: 0.1695\n",
      "Epoch [11/100], Loss: 0.1692\n",
      "Epoch [12/100], Loss: 0.1686\n",
      "Epoch [13/100], Loss: 0.1682\n",
      "Epoch [14/100], Loss: 0.1681\n",
      "Epoch [15/100], Loss: 0.1676\n",
      "Epoch [16/100], Loss: 0.1675\n",
      "Epoch [17/100], Loss: 0.1670\n",
      "Epoch [18/100], Loss: 0.1668\n",
      "Epoch [19/100], Loss: 0.1666\n",
      "Epoch [20/100], Loss: 0.1664\n",
      "Epoch [21/100], Loss: 0.1662\n",
      "Epoch [22/100], Loss: 0.1661\n",
      "Epoch [23/100], Loss: 0.1658\n",
      "Epoch [24/100], Loss: 0.1657\n",
      "Epoch [25/100], Loss: 0.1656\n",
      "Epoch [26/100], Loss: 0.1658\n",
      "Epoch [27/100], Loss: 0.1658\n",
      "Epoch [28/100], Loss: 0.1652\n",
      "Epoch [29/100], Loss: 0.1653\n",
      "Epoch [30/100], Loss: 0.1649\n",
      "Epoch [31/100], Loss: 0.1649\n",
      "Epoch [32/100], Loss: 0.1649\n",
      "Epoch [33/100], Loss: 0.1649\n",
      "Epoch [34/100], Loss: 0.1649\n",
      "Epoch [35/100], Loss: 0.1646\n",
      "Epoch [36/100], Loss: 0.1645\n",
      "Epoch [37/100], Loss: 0.1644\n",
      "Epoch [38/100], Loss: 0.1644\n",
      "Epoch [39/100], Loss: 0.1646\n",
      "Epoch [40/100], Loss: 0.1643\n",
      "Epoch [41/100], Loss: 0.1642\n",
      "Epoch [42/100], Loss: 0.1643\n",
      "Epoch [43/100], Loss: 0.1642\n",
      "Epoch [44/100], Loss: 0.1640\n",
      "Epoch [45/100], Loss: 0.1641\n",
      "Epoch [46/100], Loss: 0.1641\n",
      "Epoch [47/100], Loss: 0.1641\n",
      "Epoch [48/100], Loss: 0.1640\n",
      "Epoch [49/100], Loss: 0.1637\n",
      "Epoch [50/100], Loss: 0.1636\n",
      "Epoch [51/100], Loss: 0.1636\n",
      "Epoch [52/100], Loss: 0.1637\n",
      "Epoch [53/100], Loss: 0.1637\n",
      "Epoch [54/100], Loss: 0.1634\n",
      "Epoch [55/100], Loss: 0.1634\n",
      "Epoch [56/100], Loss: 0.1635\n",
      "Epoch [57/100], Loss: 0.1633\n",
      "Epoch [58/100], Loss: 0.1634\n",
      "Epoch [59/100], Loss: 0.1632\n",
      "Epoch [60/100], Loss: 0.1632\n",
      "Epoch [61/100], Loss: 0.1631\n",
      "Epoch [62/100], Loss: 0.1633\n",
      "Epoch [63/100], Loss: 0.1629\n",
      "Epoch [64/100], Loss: 0.1630\n",
      "Epoch [65/100], Loss: 0.1628\n",
      "Epoch [66/100], Loss: 0.1633\n",
      "Epoch [67/100], Loss: 0.1632\n",
      "Epoch [68/100], Loss: 0.1630\n",
      "Epoch [69/100], Loss: 0.1629\n",
      "Epoch [70/100], Loss: 0.1627\n",
      "Epoch [71/100], Loss: 0.1626\n",
      "Epoch [72/100], Loss: 0.1627\n",
      "Epoch [73/100], Loss: 0.1626\n",
      "Epoch [74/100], Loss: 0.1631\n",
      "Epoch [75/100], Loss: 0.1625\n",
      "Epoch [76/100], Loss: 0.1624\n",
      "Epoch [77/100], Loss: 0.1626\n",
      "Epoch [78/100], Loss: 0.1625\n",
      "Epoch [79/100], Loss: 0.1622\n",
      "Epoch [80/100], Loss: 0.1623\n",
      "Epoch [81/100], Loss: 0.1623\n",
      "Epoch [82/100], Loss: 0.1622\n",
      "Epoch [83/100], Loss: 0.1625\n",
      "Epoch [84/100], Loss: 0.1626\n",
      "Epoch [85/100], Loss: 0.1619\n",
      "Epoch [86/100], Loss: 0.1621\n",
      "Epoch [87/100], Loss: 0.1620\n",
      "Epoch [88/100], Loss: 0.1620\n",
      "Epoch [89/100], Loss: 0.1619\n",
      "Epoch [90/100], Loss: 0.1618\n",
      "Epoch [91/100], Loss: 0.1619\n",
      "Epoch [92/100], Loss: 0.1621\n",
      "Epoch [93/100], Loss: 0.1617\n",
      "Epoch [94/100], Loss: 0.1618\n",
      "Epoch [95/100], Loss: 0.1617\n",
      "Epoch [96/100], Loss: 0.1618\n",
      "Epoch [97/100], Loss: 0.1616\n",
      "Epoch [98/100], Loss: 0.1617\n",
      "Epoch [99/100], Loss: 0.1616\n",
      "Epoch [100/100], Loss: 0.1616\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    siamese_model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in pair_loader:\n",
    "        book1_emb, book2_emb, labels = batch\n",
    "        book1_emb, book2_emb, labels = book1_emb.cuda(), book2_emb.cuda(), labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = siamese_model(book1_emb, book2_emb)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(pair_loader):.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(siamese_model.state_dict(), 'siamese_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c7122a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract book embeddings from the CNN branch of the Siamese model\n",
    "def extract_embeddings_from_model(bert_embeddings, siamese_model):\n",
    "    siamese_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        book_embeddings = siamese_model.cnn_branch(bert_embeddings.cuda())  # Pass through CNN branch\n",
    "    return book_embeddings.cpu().detach().numpy()  # Move to CPU and detach from computation graph\n",
    "\n",
    "# Use the function to get the processed embeddings\n",
    "book_embeddings = extract_embeddings_from_model(bert_embeddings, siamese_model)\n",
    "\n",
    "# Save the embeddings for future use\n",
    "torch.save(book_embeddings, 'bert_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6876a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_book_embeddings = normalize(book_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae888da",
   "metadata": {},
   "source": [
    "### Similarity by Cosine Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ca896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_sim_matrix = cosine_similarity(normalized_book_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4e201",
   "metadata": {},
   "source": [
    "### Dumps cosine similarities matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bb1e8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 / 51 chunks to D:\\pickles\\dumps\\recommendation_system\\saved_matrices\\cosine_sim_matrix_chunk_50.pkl\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# folder_path = r'D:\\pickles\\dumps\\recommendation_system\\saved_matrices'\n",
    "# os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# # Save the matrix in chunks\n",
    "# chunk_size = 1000\n",
    "# num_chunks = len(cosine_sim_matrix) // chunk_size + 1\n",
    "\n",
    "# for i in range(num_chunks):\n",
    "#     chunk = cosine_sim_matrix[i * chunk_size: (i + 1) * chunk_size]\n",
    "#     file_path = os.path.join(folder_path, f'cosine_sim_matrix_chunk_{i}.pkl')\n",
    "#     with open(file_path, 'wb') as f:\n",
    "#         pickle.dump(chunk, f)\n",
    "#     clear_output(wait=True)\n",
    "#     print(f'Saved {i} / {num_chunks} chunks to {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74321f",
   "metadata": {},
   "source": [
    "### Load cosine similarities matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd3d48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "\n",
    "# folder_path = r'D:\\pickles\\dumps\\recommendation_system\\saved_matrices'\n",
    "# file_count = len(os.listdir(folder_path))\n",
    "\n",
    "# cosine_sim_matrix = []\n",
    "\n",
    "# for i in range(file_count):\n",
    "#     file_path = os.path.join(folder_path, f'cosine_sim_matrix_chunk_{i}.pkl')\n",
    "#     with open(file_path, 'rb') as f:\n",
    "#         chunk = pickle.load(f)\n",
    "#         cosine_sim_matrix.append(chunk)\n",
    "#     clear_output(wait=True)\n",
    "#     print(f'Loaded {i} / {file_count} chunks from {file_path}')\n",
    "\n",
    "# # Convert the list of chunks into a full matrix\n",
    "# cosine_sim_matrix = np.concatenate(cosine_sim_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88cd5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the book recommendation function based on cosine similarity\n",
    "# def recommend_books_by_cosine(book_title, threshold, cosine_sim_matrix):\n",
    "#     # Get the index of the book that matches the title\n",
    "#     idx = data[data['Title'] == book_title].index[0]\n",
    "\n",
    "#     # Get the cosine similarity scores for all books with this book\n",
    "#     sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "\n",
    "#     # Sort the books based on the similarity scores\n",
    "#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "#     sim_scores = [(i, \"{:.5f}\".format(score)) for i, score in sim_scores if score >= threshold]\n",
    "\n",
    "#     # Get the book titles and their similarity scores\n",
    "#     book_recommendations = [(data['Title'].iloc[i[0]], i[1]) for i in sim_scores]\n",
    "\n",
    "#     return book_recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499fd0b",
   "metadata": {},
   "source": [
    "### Similarity by Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5560ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# # Compute Euclidean distance matrix\n",
    "# euclidean_dist_matrix = euclidean_distances(normalized_book_embeddings)\n",
    "\n",
    "# # Define a recommendation function based on Euclidean distance\n",
    "# def recommend_books_by_euclidean(book_title, threshold, euclidean_dist_matrix):\n",
    "#     idx = data[data['Title'] == book_title].index[0]\n",
    "    \n",
    "#     dist_scores = list(enumerate(euclidean_dist_matrix[idx]))\n",
    "    \n",
    "#     # Sort the books based on Euclidean distance (lower is more similar)\n",
    "#     dist_scores = sorted(dist_scores, key=lambda x: x[1])\n",
    "    \n",
    "    \n",
    "#     # Filter recommendations based on threshold (optional)\n",
    "#     dist_scores = [(i, \"{:.5f}\".format(score)) for i, score in dist_scores if score <= threshold]\n",
    "#     book_recommendations = [(data['Title'].iloc[i[0]], i[1]) for i in dist_scores]\n",
    "\n",
    "#     return book_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d38b53b",
   "metadata": {},
   "source": [
    "### Similarity by Manhattan Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a3731b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "manhattan_dist_matrix = cdist(book_embeddings, book_embeddings, metric='cityblock') # Manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6278d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_by_manhattan(book_title, threshold, manhattan_dist_matrix):\n",
    "    idx = data[data['Title'] == book_title].index[0]\n",
    "    \n",
    "    # Compute the Manhattan distance scores\n",
    "    dist_scores = list(enumerate(manhattan_dist_matrix[idx]))\n",
    "    \n",
    "    # Sort the books based on Manhattan distance (lower distance means more similar)\n",
    "    dist_scores = sorted(dist_scores, key=lambda x: x[1], reverse=False)\n",
    "    \n",
    "    # Filter recommendations based on the threshold (optional)\n",
    "    recommendations = [(data['Title'].iloc[i], \"{:.5f}\".format(score)) \n",
    "                       for i, score in dist_scores if score <= threshold]\n",
    "    \n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01740755",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m book_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the title of a book: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m recommended_books \u001b[38;5;241m=\u001b[39m recommend_books_by_manhattan(book_title, \u001b[38;5;241m1.0\u001b[39m, manhattan_dist_matrix)\n\u001b[0;32m      6\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCounts: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(recommended_books)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m, in \u001b[0;36mrecommend_books_by_manhattan\u001b[1;34m(book_title, threshold, manhattan_dist_matrix)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecommend_books_by_manhattan\u001b[39m(book_title, threshold, manhattan_dist_matrix):\n\u001b[1;32m----> 2\u001b[0m     idx \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m book_title]\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Compute the Manhattan distance scores\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     dist_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(manhattan_dist_matrix[idx]))\n",
      "File \u001b[1;32mc:\\Users\\Kenneth\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5389\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[0;32m   5387\u001b[0m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[0;32m   5388\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key)\n\u001b[1;32m-> 5389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m getitem(key)\n\u001b[0;32m   5391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   5392\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[0;32m   5393\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[0;32m   5394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "book_title = ''\n",
    "while (book_title != 'q'):\n",
    "  clear_output(wait=True)\n",
    "  book_title = input(\"Enter the title of a book: \")\n",
    "  recommended_books = recommend_books_by_manhattan(book_title, 1.0, manhattan_dist_matrix)\n",
    "  f = open('output.txt', 'w')\n",
    "  f.write('Counts: ' + str(len(recommended_books)) + '\\n\\n')\n",
    "  for book in recommended_books:\n",
    "    f.write(book[1] + ' | ' + str(book[0]) + '\\n')\n",
    "\n",
    "  print('Found: ' + str(len(recommended_books)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
