{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a469b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import clear_output\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e49a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"books_data.csv\", nrows=50000)\n",
    "data['Title'] = data['Title'].fillna('Unknown')\n",
    "data['categories'] = data['categories'].fillna('Unknown')\n",
    "data['description'] = data['description'].fillna('')\n",
    "data['description'] = data['description'].apply(lambda x: x.lower())\n",
    "data['book_content'] = data['Title'] + ' ' + data['description'] + ' ' + data['authors'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '') + ' ' + data['categories'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "data['book_content'] = data['book_content'].str.replace(r'[^\\w\\s]', '', regex=True).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac14c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['book_content'].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642ea109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer and model\n",
    "logging.set_verbosity_error()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f766b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, max_len=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65579b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenize_texts(data['book_content'], max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc06129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_embeddings_in_batches(tokenized_data, bert_model, batch_size=32, device='cuda'):\n",
    "    # Move the model to the specified device (GPU or CPU)\n",
    "    bert_model = bert_model.to(device)\n",
    "    \n",
    "    # Initialize an empty list to store the embeddings\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Calculate total batches\n",
    "    total_samples = tokenized_data['input_ids'].shape[0]\n",
    "    \n",
    "    for start_idx in range(0, total_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_samples)\n",
    "        \n",
    "        # Slice batch input_ids and attention_mask\n",
    "        input_ids_batch = tokenized_data['input_ids'][start_idx:end_idx].to(device)  # Move to the same device\n",
    "        attention_mask_batch = tokenized_data['attention_mask'][start_idx:end_idx].to(device)  # Move to the same device\n",
    "\n",
    "        # Get BERT embeddings without computing gradients\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = bert_model(input_ids=input_ids_batch, attention_mask=attention_mask_batch)[1]\n",
    "        \n",
    "        # Move embeddings back to CPU to save GPU memory\n",
    "        all_embeddings.append(batch_embeddings.cpu())\n",
    "        \n",
    "        # Optionally clear cache to free memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all batch embeddings into a single tensor\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Use the function to generate embeddings in batches\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_embeddings = generate_bert_embeddings_in_batches(tokenized_data, bert_model, batch_size=32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423d5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, data, bert_embeddings):\n",
    "        self.data = data\n",
    "        self.bert_embeddings = bert_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        book1_emb = self.bert_embeddings[idx]\n",
    "        book2_emb = self.bert_embeddings[(idx + 1) % len(self.data)]  # Pair with next item\n",
    "        label = 1 if self.data['categories'].iloc[idx] == self.data['categories'].iloc[(idx + 1) % len(self.data)] else 0\n",
    "        return book1_emb, book2_emb, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2982375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNBranch, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 512)  # First dense layer, increased number of units\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Dropout to prevent overfitting\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)  # Second dense layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)  # Dropout again\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 128)  # Third dense layer (matches original)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb9f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn_branch = CNNBranch()\n",
    "        self.fc1 = nn.Linear(768, 128)  # Assuming BERT output size is 768\n",
    "        self.fc2 = nn.Linear(128, 64)   # Reducing to 64 dimensions\n",
    "        self.fc3 = nn.Linear(64 * 2, 2)  # Concatenating two 64-dim vectors, and output size 2 for binary classification\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        concatenated = torch.cat((output1, output2), dim=1)  # Concatenating along the feature dimension (dim=1)\n",
    "        output = self.fc3(concatenated)\n",
    "        return output\n",
    "\n",
    "# Dataset and DataLoader\n",
    "pair_dataset = PairDataset(data, bert_embeddings)\n",
    "pair_loader = DataLoader(pair_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the Siamese model, loss function, and optimizer\n",
    "siamese_model = SiameseNetwork().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(siamese_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    siamese_model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in pair_loader:\n",
    "        book1_emb, book2_emb, labels = batch\n",
    "        book1_emb, book2_emb, labels = book1_emb.cuda(), book2_emb.cuda(), labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = siamese_model(book1_emb, book2_emb)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(pair_loader):.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(siamese_model.state_dict(), 'siamese_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c7122a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract book embeddings from the CNN branch of the Siamese model\n",
    "def extract_embeddings_from_model(bert_embeddings, siamese_model):\n",
    "    siamese_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        book_embeddings = siamese_model.cnn_branch(bert_embeddings.cuda())  # Pass through CNN branch\n",
    "    return book_embeddings.cpu().detach().numpy()  # Move to CPU and detach from computation graph\n",
    "\n",
    "# Use the function to get the processed embeddings\n",
    "book_embeddings = extract_embeddings_from_model(bert_embeddings, siamese_model)\n",
    "\n",
    "# Save the embeddings for future use\n",
    "torch.save(book_embeddings, 'bert_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6876a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_book_embeddings = normalize(book_embeddings)\n",
    "cosine_sim_matrix = cosine_similarity(normalized_book_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887b43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b4e201",
   "metadata": {},
   "source": [
    "### Dumps cosine similarities matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "folder_path = r'D:\\pickles\\dumps\\recommendation_system\\saved_matrices'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Save the matrix in chunks\n",
    "chunk_size = 1000\n",
    "num_chunks = len(cosine_sim_matrix) // chunk_size + 1\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    chunk = cosine_sim_matrix[i * chunk_size: (i + 1) * chunk_size]\n",
    "    file_path = os.path.join(folder_path, f'cosine_sim_matrix_chunk_{i}.pkl')\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(chunk, f)\n",
    "    clear_output(wait=True)\n",
    "    print(f'Saved {i} / {num_chunks} chunks to {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74321f",
   "metadata": {},
   "source": [
    "### Load cosine similarities matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "folder_path = r'D:\\pickles\\dumps\\recommendation_system\\saved_matrices'\n",
    "file_count = len(os.listdir(folder_path))\n",
    "\n",
    "cosine_sim_matrix = []\n",
    "\n",
    "for i in range(file_count):\n",
    "    file_path = os.path.join(folder_path, f'cosine_sim_matrix_chunk_{i}.pkl')\n",
    "    with open(file_path, 'rb') as f:\n",
    "        chunk = pickle.load(f)\n",
    "        cosine_sim_matrix.append(chunk)\n",
    "    clear_output(wait=True)\n",
    "    print(f'Loaded {i} / {file_count} chunks from {file_path}')\n",
    "\n",
    "# Convert the list of chunks into a full matrix\n",
    "cosine_sim_matrix = np.concatenate(cosine_sim_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88cd5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the book recommendation function based on cosine similarity\n",
    "def recommend_books(book_title, threshold, cosine_sim_matrix):\n",
    "    # Get the index of the book that matches the title\n",
    "    idx = data[data['Title'] == book_title].index[0]\n",
    "\n",
    "    # Get the cosine similarity scores for all books with this book\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "\n",
    "    # Sort the books based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = [(i, \"{:.5f}\".format(score)) for i, score in sim_scores if score >= threshold]\n",
    "\n",
    "    # Get the book titles and their similarity scores\n",
    "    book_recommendations = [(data['Title'].iloc[i[0]], i[1]) for i in sim_scores]\n",
    "\n",
    "    return book_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5560ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Compute Euclidean distance matrix\n",
    "euclidean_dist_matrix = euclidean_distances(normalized_book_embeddings)\n",
    "\n",
    "# Define a recommendation function based on Euclidean distance\n",
    "def recommend_books_by_euclidean(book_title, threshold, euclidean_dist_matrix):\n",
    "    idx = data[data['Title'] == book_title].index[0]\n",
    "    \n",
    "    dist_scores = list(enumerate(euclidean_dist_matrix[idx]))\n",
    "    \n",
    "    # Sort the books based on Euclidean distance (lower is more similar)\n",
    "    dist_scores = sorted(dist_scores, key=lambda x: x[1])\n",
    "    \n",
    "    \n",
    "    # Filter recommendations based on threshold (optional)\n",
    "    dist_scores = [(i, \"{:.5f}\".format(score)) for i, score in dist_scores if score <= threshold]\n",
    "    book_recommendations = [(data['Title'].iloc[i[0]], i[1]) for i in dist_scores]\n",
    "\n",
    "    return book_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01740755",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_title = ''\n",
    "while (book_title != 'q'):\n",
    "  clear_output(wait=True)\n",
    "  book_title = input(\"Enter the title of a book: \")\n",
    "  recommended_books = recommend_books(book_title, 0.95, cosine_sim_matrix)\n",
    "  f = open('output.txt', 'w')\n",
    "  f.write('Counts: ' + str(len(recommended_books)) + '\\n\\n')\n",
    "  for book in recommended_books:\n",
    "    f.write(book[1] + ' | ' + str(book[0]) + '\\n')\n",
    "\n",
    "  print('Found: ' + str(len(recommended_books)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
