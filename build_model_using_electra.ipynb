{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70954691",
   "metadata": {},
   "source": [
    "## Please run this code only if you want to regenerate the embedding file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c12165",
   "metadata": {},
   "source": [
    "### Refer to pytorch documentation to find the pytorch version that matches with your CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a469b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ElectraTokenizer, ElectraModel\n",
    "import pandas as pd\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e49a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"books_data.csv\", nrows=40000)\n",
    "data['Title'] = data['Title'].fillna('Unknown')\n",
    "data['categories'] = data['categories'].fillna('Unknown')\n",
    "data['description'] = data['description'].fillna('')\n",
    "data['description'] = data['description'].apply(lambda x: x.lower())\n",
    "data['book_content'] = (\n",
    "    (data['Title'] + ' ') * 2\n",
    "    + data['description'] + ' '\n",
    "    + data['authors'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '') + ' '\n",
    "    + data['categories'].apply(lambda x: ' '.join(x) * 5 if isinstance(x, list) else '')\n",
    ")\n",
    "data['book_content'] = data['book_content'].str.replace(r'[^\\w\\s]', '', regex=True).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac14c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17476    the rehabilitation of myth vicos new science t...\n",
      "21562    the eighth night of creation the eighth night ...\n",
      "19183    nicholas winton and the rescued generation sav...\n",
      "37643    review of sports medicine  arthroscopy review ...\n",
      "39353    shifra steins day trips from houston getaways ...\n",
      "Name: book_content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['book_content'].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642ea109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a95c185cb749209910e5a50ece7233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9fad868f66445a869ed2fbf8c57b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9b37aa202c462881bda2a0b5512779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c171985fe71f4fca9f9f7e2db11a04f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/668 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014c9b5e663c4cd8ba32d173d3de7a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-large-discriminator')\n",
    "electra_model = ElectraModel.from_pretrained('google/electra-large-discriminator').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f766b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, max_len=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "tokenized_data = tokenize_texts(data['book_content'], max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc06129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40000, 1024])\n"
     ]
    }
   ],
   "source": [
    "def generate_electra_embeddings_in_batches(tokenized_data, electra_model, batch_size=32, device='cuda'):\n",
    "    electra_model = electra_model.to(device)\n",
    "    all_embeddings = []\n",
    "    total_samples = tokenized_data['input_ids'].shape[0]\n",
    "\n",
    "    for start_idx in range(0, total_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_samples)\n",
    "        input_ids_batch = tokenized_data['input_ids'][start_idx:end_idx].to(device)\n",
    "        attention_mask_batch = tokenized_data['attention_mask'][start_idx:end_idx].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = electra_model(input_ids=input_ids_batch, attention_mask=attention_mask_batch).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        all_embeddings.append(batch_embeddings.cpu())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "electra_embeddings = generate_electra_embeddings_in_batches(tokenized_data, electra_model, batch_size=32, device=device)\n",
    "print(electra_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "423d5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, data, embeddings):\n",
    "        self.data = data\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        book1_emb = self.embeddings[idx]\n",
    "        book2_emb = self.embeddings[(idx + 1) % len(self.data)]\n",
    "        label = 1 if self.data['categories'].iloc[idx] == self.data['categories'].iloc[(idx + 1) % len(self.data)] else 0\n",
    "        return book1_emb, book2_emb, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2982375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBranch, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb9f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn_branch = CustomBranch()\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64 * 2, 2)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        concatenated = torch.cat((output1, output2), dim=1)\n",
    "        output = self.fc3(concatenated)\n",
    "        return output\n",
    "\n",
    "# Dataset and DataLoader\n",
    "pair_dataset = PairDataset(data, electra_embeddings)\n",
    "pair_loader = DataLoader(pair_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train Siamese Network\n",
    "siamese_model = SiameseNetwork().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(siamese_model.parameters(), lr=1e-5, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72eb34ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.3910\n",
      "Epoch [2/100], Loss: 0.1696\n",
      "Epoch [3/100], Loss: 0.1680\n",
      "Epoch [4/100], Loss: 0.1675\n",
      "Epoch [5/100], Loss: 0.1670\n",
      "Epoch [6/100], Loss: 0.1666\n",
      "Epoch [7/100], Loss: 0.1663\n",
      "Epoch [8/100], Loss: 0.1659\n",
      "Epoch [9/100], Loss: 0.1656\n",
      "Epoch [10/100], Loss: 0.1653\n",
      "Epoch [11/100], Loss: 0.1649\n",
      "Epoch [12/100], Loss: 0.1645\n",
      "Epoch [13/100], Loss: 0.1642\n",
      "Epoch [14/100], Loss: 0.1639\n",
      "Epoch [15/100], Loss: 0.1637\n",
      "Epoch [16/100], Loss: 0.1633\n",
      "Epoch [17/100], Loss: 0.1631\n",
      "Epoch [18/100], Loss: 0.1629\n",
      "Epoch [19/100], Loss: 0.1625\n",
      "Epoch [20/100], Loss: 0.1623\n",
      "Epoch [21/100], Loss: 0.1620\n",
      "Epoch [22/100], Loss: 0.1617\n",
      "Epoch [23/100], Loss: 0.1616\n",
      "Epoch [24/100], Loss: 0.1613\n",
      "Epoch [25/100], Loss: 0.1611\n",
      "Epoch [26/100], Loss: 0.1609\n",
      "Epoch [27/100], Loss: 0.1607\n",
      "Epoch [28/100], Loss: 0.1605\n",
      "Epoch [29/100], Loss: 0.1603\n",
      "Epoch [30/100], Loss: 0.1601\n",
      "Epoch [31/100], Loss: 0.1599\n",
      "Epoch [32/100], Loss: 0.1598\n",
      "Epoch [33/100], Loss: 0.1595\n",
      "Epoch [34/100], Loss: 0.1595\n",
      "Epoch [35/100], Loss: 0.1593\n",
      "Epoch [36/100], Loss: 0.1591\n",
      "Epoch [37/100], Loss: 0.1590\n",
      "Epoch [38/100], Loss: 0.1588\n",
      "Epoch [39/100], Loss: 0.1587\n",
      "Epoch [40/100], Loss: 0.1586\n",
      "Epoch [41/100], Loss: 0.1585\n",
      "Epoch [42/100], Loss: 0.1583\n",
      "Epoch [43/100], Loss: 0.1583\n",
      "Epoch [44/100], Loss: 0.1581\n",
      "Epoch [45/100], Loss: 0.1580\n",
      "Epoch [46/100], Loss: 0.1579\n",
      "Epoch [47/100], Loss: 0.1577\n",
      "Epoch [48/100], Loss: 0.1578\n",
      "Epoch [49/100], Loss: 0.1576\n",
      "Epoch [50/100], Loss: 0.1575\n",
      "Epoch [51/100], Loss: 0.1575\n",
      "Epoch [52/100], Loss: 0.1573\n",
      "Epoch [53/100], Loss: 0.1572\n",
      "Epoch [54/100], Loss: 0.1571\n",
      "Epoch [55/100], Loss: 0.1570\n",
      "Epoch [56/100], Loss: 0.1569\n",
      "Epoch [57/100], Loss: 0.1568\n",
      "Epoch [58/100], Loss: 0.1568\n",
      "Epoch [59/100], Loss: 0.1566\n",
      "Epoch [60/100], Loss: 0.1565\n",
      "Epoch [61/100], Loss: 0.1565\n",
      "Epoch [62/100], Loss: 0.1563\n",
      "Epoch [63/100], Loss: 0.1563\n",
      "Epoch [64/100], Loss: 0.1562\n",
      "Epoch [65/100], Loss: 0.1561\n",
      "Epoch [66/100], Loss: 0.1559\n",
      "Epoch [67/100], Loss: 0.1559\n",
      "Epoch [68/100], Loss: 0.1558\n",
      "Epoch [69/100], Loss: 0.1557\n",
      "Epoch [70/100], Loss: 0.1556\n",
      "Epoch [71/100], Loss: 0.1556\n",
      "Epoch [72/100], Loss: 0.1554\n",
      "Epoch [73/100], Loss: 0.1553\n",
      "Epoch [74/100], Loss: 0.1553\n",
      "Epoch [75/100], Loss: 0.1551\n",
      "Epoch [76/100], Loss: 0.1551\n",
      "Epoch [77/100], Loss: 0.1549\n",
      "Epoch [78/100], Loss: 0.1548\n",
      "Epoch [79/100], Loss: 0.1547\n",
      "Epoch [80/100], Loss: 0.1546\n",
      "Epoch [81/100], Loss: 0.1545\n",
      "Epoch [82/100], Loss: 0.1544\n",
      "Epoch [83/100], Loss: 0.1543\n",
      "Epoch [84/100], Loss: 0.1542\n",
      "Epoch [85/100], Loss: 0.1541\n",
      "Epoch [86/100], Loss: 0.1541\n",
      "Epoch [87/100], Loss: 0.1539\n",
      "Epoch [88/100], Loss: 0.1539\n",
      "Epoch [89/100], Loss: 0.1538\n",
      "Epoch [90/100], Loss: 0.1537\n",
      "Epoch [91/100], Loss: 0.1536\n",
      "Epoch [92/100], Loss: 0.1535\n",
      "Epoch [93/100], Loss: 0.1534\n",
      "Epoch [94/100], Loss: 0.1533\n",
      "Epoch [95/100], Loss: 0.1531\n",
      "Epoch [96/100], Loss: 0.1530\n",
      "Epoch [97/100], Loss: 0.1530\n",
      "Epoch [98/100], Loss: 0.1529\n",
      "Epoch [99/100], Loss: 0.1528\n",
      "Epoch [100/100], Loss: 0.1526\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    siamese_model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in pair_loader:\n",
    "        book1_emb, book2_emb, labels = batch\n",
    "        book1_emb, book2_emb, labels = book1_emb.cuda(), book2_emb.cuda(), labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = siamese_model(book1_emb, book2_emb)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(pair_loader):.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(siamese_model.state_dict(), 'siamese_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c7122a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_from_model(electra_embeddings, siamese_model):\n",
    "    siamese_model.eval()\n",
    "    with torch.no_grad():\n",
    "        book_embeddings = siamese_model.cnn_branch(electra_embeddings.cuda())\n",
    "    return book_embeddings.cpu().detach().numpy()\n",
    "\n",
    "book_embeddings = extract_embeddings_from_model(electra_embeddings, siamese_model)\n",
    "torch.save(book_embeddings, 'electra_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4e201",
   "metadata": {},
   "source": [
    "### Dumps cosine similarities matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e17ad830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# folder_path = r'dumped_matrices/chebyshev_distance'\n",
    "# os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# # Save the matrix in chunks\n",
    "# chunk_size = 2048\n",
    "# num_chunks = len(manhattan_dist_matrix) // chunk_size + 1\n",
    "\n",
    "# for i in range(num_chunks):\n",
    "#     chunk = manhattan_dist_matrix[i * chunk_size: (i + 1) * chunk_size]\n",
    "#     file_path = os.path.join(folder_path, f'chebyshev_matrix_chunk_{i}.pkl')\n",
    "#     with open(file_path, 'wb') as f:\n",
    "#         pickle.dump(chunk, f)\n",
    "#     clear_output(wait=True)\n",
    "#     print(f'Saved {i} / {num_chunks} chunks to {file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
